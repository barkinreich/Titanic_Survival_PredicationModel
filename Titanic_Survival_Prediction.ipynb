{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* Name 1: Bar Kinreich 313178345\n",
    "* Name 2: Neta Dor 206759623\n",
    "\n",
    "## To be submitted by: 20.2.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic survival prediction\n",
    "\n",
    "\n",
    "\n",
    "In this part we will attempt to predict who survived on the Titanic. The data set we use has the following vairbales (features/attributes):\n",
    "\n",
    "\n",
    "|  Variable   |          Definition          |              Key/Values                 |\n",
    "|:-----------:|:----------------------------:|:---------------------------------------:|\n",
    "| PassengerId | Index                        |integer                                  |\n",
    "| Pclass      | Ticket class                 |1=1st, 2=2nd, 3=3rd                      |\n",
    "| Name        | Name of passenger            |string                                   |\n",
    "| Sex         | Sex                          |male, female                             |\n",
    "| Age         | Age in years                 |integer                                  |\n",
    "| SibSp       | # of siblings/spouses aboard |integer                                  |\n",
    "| Parch       | # of parents/children aboard |integer                                  |\n",
    "| Ticket      | Ticket number                |string                                   |\n",
    "| Fare        | Ticket fare                  |float                                    |\n",
    "| Cabin       | Cabin number                 |a code                                   |\n",
    "| Embarked    | Port of Embarkation          |C=Cherbourg, Q=Queenstown, S=Southampton |\n",
    "| **Survived**| Predicted varibale           |0=No, 1=Yes                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "\n",
    "This part of the project is a competetive one. The goal is to produce the best prediciton you can\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**Methodogology**\n",
    "\n",
    "So far you only know few methods for classification: KNN, logistic regression and SVM. You can use each one of them. You can also use linear regression, but then you need to convert the output to 0s and 1s (this is not a straight forward use of linear regression but a possible one). You may want to choose which features to use (it could be that some features are not useful). Further, some features have missing values. You will need to decide how to handle this (for instance: drop rows with missing values, place an avergae value in those rows, or some other method of your choosing). Another matter to consider is handling non-numeric values. For example, sex is non-numeric. You may choose to drop non-numeric features, or you could convert them to numeric values (if such conversion makes sense).\n",
    "Also you will need to consider splitting the data into a training set and a test set so as to avoid tailoring the solution (overfitting) to the data you have.\n",
    "<br><br>\n",
    "Bottom line: use everything we talked about in class in order to learn the best model.\n",
    "\n",
    "\n",
    "**Model Output**\n",
    "\n",
    "Your model needs to produce a prediciton for each data sample (row), which is 0 (did not survive) or 1 (survived). \n",
    "<br><br>\n",
    "\n",
    "**Scoring your model**\n",
    "\n",
    "Your model performance will be assessed on test data that is not available to you. As mentioned above, this part of the project is a competition, where the goal is to achieve highest model accuracy\n",
    "\n",
    "<br><br>\n",
    "**Final Note**\n",
    "\n",
    "This part of the project is open ended, in that you are not given small and specific tasks. However, you are already familiar with all the components needed to succeed. Specifically, reading data into pandas dataframe, dropping columns, dropping rows, changing value of features, splitting data into train and test subsets, performing model using sklearn library, and using cross validation. So don't panic...\n",
    "\n",
    "\n",
    "GOOD LUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imporing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# data processing\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"titanic_train.csv\")\n",
    "\n",
    "# Data Exploration\n",
    "print(\"Info: \\n\")\n",
    "print(titanic_data.info())\n",
    "print(\"\\n Describe: \\n\")\n",
    "print(titanic_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values explotarion\n",
    "total = titanic_data.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = titanic_data.isnull().sum()/titanic_data.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(5)\n",
    "\n",
    "# The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, \n",
    "# to deal with the ‘Age’ feature, which has 177 missing values. \n",
    "#The ‘Cabin’ feature needs further investigation, but it looks like that we might want to drop it from the dataset, \n",
    "# since 77 % of it are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find means of each feature in order to compare between different genders\n",
    "titanic_data.groupby('Sex').mean()\n",
    "\n",
    "# We can see that males had a much lower chance of survival then women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will try to understand which features could contribute to a high survival rate by using plots\n",
    "\n",
    "# Age and Sex\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
    "women = titanic_data[titanic_data['Sex']=='female']\n",
    "men = titanic_data[titanic_data['Sex']=='male']\n",
    "ax = sns.histplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False, color = 'skyblue')\n",
    "ax = sns.histplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False, color = 'pink')\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.histplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False, color = 'skyblue')\n",
    "ax = sns.histplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False, color = 'pink')\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')\n",
    "\n",
    "# Since there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, I will create age groups later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked, Pclass and Sex:\n",
    "FacetGrid = sns.FacetGrid(titanic_data, row='Embarked', height=4.5, aspect=1.6)\n",
    "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\n",
    "FacetGrid.add_legend()\n",
    "\n",
    "# We can see that women who embarked in port Q had a very high survival rate, while men who embarked in port C had a high survival rate. It seems that one gender's survival comes at the price of the other, as they have an inverse coorelation between them.\n",
    "# This is except for both women and men that embarked in port S and were in class 3, where both genders had a low survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass:\n",
    "sns.barplot(x='Pclass', y='Survived', data=titanic_data)\n",
    "\n",
    "# We can see that there is a very high correlation between Pclass and Survived, those in class 1 had a much higher survival rate then those in class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like to delete the columns that we believe don't contribute to the survival target\n",
    "titanic_data = titanic_data.drop(['PassengerId'], axis=1)\n",
    "titanic_data = titanic_data.drop(['Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin \n",
    "# A cabin number contains the letter refers to the deck. \n",
    "# Therefore we’re going to extract these and create a new feature, that contains a persons deck. \n",
    "# Afterwords we will convert the feature into a numeric variable. \n",
    "# The missing values will be converted to zero.\n",
    "\n",
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "\n",
    "titanic_data['Cabin'] = titanic_data['Cabin'].fillna(\"U0\")\n",
    "titanic_data['Deck'] = titanic_data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "titanic_data['Deck'] = titanic_data['Deck'].map(deck)\n",
    "titanic_data['Deck'] = titanic_data['Deck'].fillna(0)\n",
    "titanic_data['Deck'] = titanic_data['Deck'].astype(int)\n",
    "# we can now drop the cabin feature\n",
    "titanic_data = titanic_data.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "# we create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.\n",
    "\n",
    "mean = titanic_data[\"Age\"].mean()\n",
    "std = titanic_data[\"Age\"].std()\n",
    "is_null = titanic_data[\"Age\"].isnull().sum()\n",
    "# compute random numbers between the mean, std and is_null\n",
    "rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "# fill NaN values in Age column with random values generated\n",
    "age_slice = titanic_data[\"Age\"].copy()\n",
    "age_slice[np.isnan(age_slice)] = rand_age\n",
    "titanic_data[\"Age\"] = age_slice\n",
    "titanic_data[\"Age\"] = titanic_data[\"Age\"].astype(int)\n",
    "\n",
    "# check that we got rid of the nulls\n",
    "titanic_data[\"Age\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n",
    "# Embarked feature has only 2 missing values, we will  fill these with the most common one.\n",
    "\n",
    "titanic_data['Embarked'].describe()\n",
    "# because the top value is 'S', we will fill the missing values with it.\n",
    "titanic_data['Embarked'] = titanic_data['Embarked'].fillna('S')\n",
    "\n",
    "# check that we got rid of the nulls\n",
    "titanic_data[\"Embarked\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to see how to combine the \"Title\" coloumn and change it to categorial, we will first check what titles are in our dataset\n",
    "\n",
    "titanic_data['Title'] = titanic_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "pd.crosstab(titanic_data['Title'], titanic_data['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name \n",
    "# We created a new feature called Title from the Name feature\n",
    "\n",
    "\n",
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "# extract titles\n",
    "titanic_data['Title'] = titanic_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "# replace titles with a more common title or as Other\n",
    "titanic_data['Title'] = titanic_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                        'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "titanic_data['Title'] = titanic_data['Title'].replace('Mlle', 'Miss')\n",
    "titanic_data['Title'] = titanic_data['Title'].replace('Ms', 'Miss')\n",
    "titanic_data['Title'] = titanic_data['Title'].replace('Mme', 'Mrs')\n",
    "# convert titles into numbers\n",
    "titanic_data['Title'] = titanic_data['Title'].map(titles)\n",
    "# filling NaN with 0, to get safe\n",
    "titanic_data['Title'] = titanic_data['Title'].fillna(0)\n",
    "titanic_data = titanic_data.drop(['Name'], axis=1)\n",
    "\n",
    "# check that we got rid of the nulls\n",
    "titanic_data[\"Title\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex\n",
    "# Convert ‘Sex’ feature into numeric: male = 0, female = 1\n",
    "\n",
    "genders = {\"male\": 0, \"female\": 1}\n",
    "\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n",
    "ports = {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "\n",
    "titanic_data['Embarked'] = titanic_data['Embarked'].map(ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp and Parch\n",
    "# combined feature, that shows the total number of relatives, a person has on the Titanic.\n",
    "#  I will create it below and also a feature that shows if someone is not alone.\n",
    "\n",
    "titanic_data['relatives'] = titanic_data['SibSp'] + titanic_data['Parch']\n",
    "titanic_data.loc[titanic_data['relatives'] > 0, 'not_alone'] = 0\n",
    "titanic_data.loc[titanic_data['relatives'] == 0, 'not_alone'] = 1\n",
    "titanic_data['not_alone'] = titanic_data['not_alone'].astype(int)\n",
    "\n",
    "titanic_data['relatives'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "# According to the graph we did before, there seems to be certain age groups that have a higher chance for survival, so we will use them in order to create the age groups. \n",
    "# Also, we will want to make sure that the data is evenly distributed so that a large part of our data doesn't fall into one category\n",
    "# We decided not to use qcut as the ages have meaning (infants vs. children vs. adults vs. senr)\n",
    "\n",
    "titanic_data['Age'] = titanic_data['Age'].astype(int)\n",
    "titanic_data.loc[ titanic_data['Age'] <= 11, 'Age'] = 1\n",
    "titanic_data.loc[(titanic_data['Age'] > 11) & (titanic_data['Age'] <= 18), 'Age'] = 2\n",
    "titanic_data.loc[(titanic_data['Age'] > 18) & (titanic_data['Age'] <= 24), 'Age'] = 3\n",
    "titanic_data.loc[(titanic_data['Age'] > 24) & (titanic_data['Age'] <= 30), 'Age'] = 4\n",
    "titanic_data.loc[(titanic_data['Age'] > 30) & (titanic_data['Age'] <= 37), 'Age'] = 5\n",
    "titanic_data.loc[(titanic_data['Age'] > 37) & (titanic_data['Age'] <= 45), 'Age'] = 6\n",
    "titanic_data.loc[ titanic_data['Age'] > 45, 'Age'] = 7\n",
    "\n",
    "# let's see how it's distributed \n",
    "titanic_data['Age'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "# we will use 'qcut' to help us decide the categories for the fare\n",
    "pd.qcut(titanic_data[\"Fare\"],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.loc[ titanic_data['Fare'] <= 7.775, 'Fare'] = 1\n",
    "titanic_data.loc[(titanic_data['Fare'] > 7.775) & (titanic_data['Fare'] <= 8.458), 'Fare'] = 2\n",
    "titanic_data.loc[(titanic_data['Fare'] > 8.458) & (titanic_data['Fare'] <= 14.2), 'Fare']   = 3\n",
    "titanic_data.loc[(titanic_data['Fare'] > 14.2) & (titanic_data['Fare'] <= 26.25), 'Fare']   = 4\n",
    "titanic_data.loc[(titanic_data['Fare'] > 26.25) & (titanic_data['Fare'] <= 53.1), 'Fare']   = 5\n",
    "titanic_data.loc[ titanic_data['Fare'] > 53.1, 'Fare'] = 6\n",
    "titanic_data['Fare'] = titanic_data['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Fare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a heatmap to understand what features has strong correlation, to make a new feature out of them\n",
    "sns.heatmap(titanic_data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw that Sex and Title has strong correlation with the Survived target, therefore we decided to create a new feature which includes both of them\n",
    "titanic_data['Sex_Title']= titanic_data['Sex'] + titanic_data['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and training data\n",
    "\n",
    "training_data = titanic_data.sample(frac=0.8, random_state=25)\n",
    "test_data = titanic_data.drop(training_data.index)\n",
    "\n",
    "#Creating X table of features and Y table of the target feature:\n",
    "X_train = training_data.drop(\"Survived\", axis=1)\n",
    "Y_train = training_data[\"Survived\"]\n",
    "X_val = test_data.drop(\"Survived\", axis=1)\n",
    "Y_val = test_data[\"Survived\"]\n",
    "\n",
    "\n",
    "# Normalization\n",
    "# z-score scaling\n",
    "zscore_scale = preprocessing.StandardScaler()\n",
    "zscore_scale.fit(X_train)\n",
    "Xtrain_zscore = zscore_scale.transform(X_train)\n",
    "Xtest_zscore = zscore_scale.transform(X_val)\n",
    "\n",
    "# min-max scaling\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_scale.fit(X_train)\n",
    "Xtrain_minmax = minmax_scale.transform(X_train)\n",
    "Xtest_minmax = minmax_scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After numerous tests we found that the min-max feature has a better accuracy then Z-score\n",
    "\n",
    "X_train = Xtrain_minmax\n",
    "X_val = Xtest_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_val)\n",
    "\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN \n",
    "knn = KNeighborsClassifier(n_neighbors = 7) \n",
    "knn.fit(X_train, Y_train)  \n",
    "Y_pred = knn.predict(X_val)  \n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "# with for loop we found that k=3 has the highest accuracy, but with KNN model we need to see how it works after doing K-folds to see that the accuracy is percised.\n",
    "# after testing with k-folds we saw that 7 brings the best scores\n",
    "\n",
    "print(classification_report(Y_val, Y_pred))\n",
    "\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = SVC()\n",
    "\n",
    "# Optimization of the hyper-parameters of the model\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "linear_svc1 = GridSearchCV(linear_svc, param_grid, refit = True, verbose = 3)\n",
    "\n",
    "linear_svc1.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "\n",
    "linear_svc1.best_params_\n",
    "\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(linear_svc1.best_estimator_)\n",
    "\n",
    "linear_svc = SVC(C=1, gamma=1,kernel=\"rbf\")\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred = linear_svc.predict(X_val)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "# print classification report\n",
    "print(classification_report(Y_val,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression'],\n",
    "    'Score': [acc_linear_svc, acc_knn, acc_log]})\n",
    "result_df = results.sort_values(by='Score', ascending=False)\n",
    "result_df = result_df.set_index('Score')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "### We can see that the model with the best performance by accuracy is the KNN model, with the highest accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(test_data):\n",
    "    titanic_test = pd.read_csv(test_data)\n",
    "    titanic_test = titanic_test.drop(['Ticket'], axis=1)\n",
    "    \n",
    "    #Missing values\n",
    "    # Deck\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    titanic_test['Cabin'] = titanic_test['Cabin'].fillna(\"U0\")\n",
    "    titanic_test['Deck'] = titanic_test['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    titanic_test['Deck'] = titanic_test['Deck'].map(deck)\n",
    "    titanic_test['Deck'] = titanic_test['Deck'].fillna(0)\n",
    "    titanic_test['Deck'] = titanic_test['Deck'].astype(int)\n",
    "    # We can now drop the cabin feature\n",
    "    titanic_test = titanic_test.drop(['Cabin'], axis=1)\n",
    "    \n",
    "    # Age\n",
    "    mean = titanic_test[\"Age\"].mean()\n",
    "    std = titanic_test[\"Age\"].std()\n",
    "    is_null = titanic_test[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = titanic_test[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    titanic_test[\"Age\"] = age_slice\n",
    "    titanic_test[\"Age\"] = titanic_test[\"Age\"].astype(int)\n",
    "    \n",
    "    #Embarked\n",
    "    titanic_test['Embarked'] = titanic_test['Embarked'].fillna('S')\n",
    "    \n",
    "    # Name \n",
    "    titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    # extract titles\n",
    "    titanic_test['Title'] = titanic_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # replace titles with a more common title or as Other\n",
    "    titanic_test['Title'] = titanic_test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "    titanic_test['Title'] = titanic_test['Title'].replace('Mlle', 'Miss')\n",
    "    titanic_test['Title'] = titanic_test['Title'].replace('Ms', 'Miss')\n",
    "    titanic_test['Title'] = titanic_test['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "    titanic_test['Title'] = titanic_test['Title'].map(titles)\n",
    "    # filling NaN with 0\n",
    "    titanic_test['Title'] = titanic_test['Title'].fillna(0)\n",
    "    titanic_test = titanic_test.drop(['Name'], axis=1)\n",
    "    \n",
    "    # Sex\n",
    "    # Convert ‘Sex’ feature into numeric: male = 0, female = 1\n",
    "\n",
    "    genders = {\"male\": 0, \"female\": 1}\n",
    "\n",
    "    titanic_test['Sex'] = titanic_test['Sex'].map(genders)\n",
    "    \n",
    "    # Embarked\n",
    "    ports = {\"S\": 1, \"C\": 2, \"Q\": 3}\n",
    "\n",
    "    titanic_test['Embarked'] = titanic_test['Embarked'].map(ports)\n",
    "    \n",
    "    # SibSp and Parch\n",
    "\n",
    "    titanic_test['relatives'] = titanic_test['SibSp'] + titanic_test['Parch']\n",
    "    titanic_test.loc[titanic_test['relatives'] > 0, 'not_alone'] = 0\n",
    "    titanic_test.loc[titanic_test['relatives'] == 0, 'not_alone'] = 1\n",
    "    titanic_test['not_alone'] = titanic_test['not_alone'].astype(int)\n",
    "    \n",
    "    # Age\n",
    "    \n",
    "    titanic_test['Age'] = titanic_test['Age'].astype(int)\n",
    "    titanic_test.loc[ titanic_test['Age'] <= 11, 'Age'] = 1\n",
    "    titanic_test.loc[(titanic_test['Age'] > 11) & (titanic_test['Age'] <= 18), 'Age'] = 2\n",
    "    titanic_test.loc[(titanic_test['Age'] > 18) & (titanic_test['Age'] <= 24), 'Age'] = 3\n",
    "    titanic_test.loc[(titanic_test['Age'] > 24) & (titanic_test['Age'] <= 30), 'Age'] = 4\n",
    "    titanic_test.loc[(titanic_test['Age'] > 30) & (titanic_test['Age'] <= 37), 'Age'] = 5\n",
    "    titanic_test.loc[(titanic_test['Age'] > 37) & (titanic_test['Age'] <= 45), 'Age'] = 6\n",
    "    titanic_test.loc[ titanic_test['Age'] > 45, 'Age'] = 7\n",
    "    \n",
    "    # Fare\n",
    "    \n",
    "    titanic_test.loc[ titanic_test['Fare'] <= 7.775, 'Fare'] = 1\n",
    "    titanic_test.loc[(titanic_test['Fare'] > 7.775) & (titanic_test['Fare'] <= 8.458), 'Fare'] = 2\n",
    "    titanic_test.loc[(titanic_test['Fare'] > 8.458) & (titanic_test['Fare'] <= 14.2), 'Fare']   = 3\n",
    "    titanic_test.loc[(titanic_test['Fare'] > 14.2) & (titanic_test['Fare'] <= 26.25), 'Fare']   = 4\n",
    "    titanic_test.loc[(titanic_test['Fare'] > 26.25) & (titanic_test['Fare'] <= 53.1), 'Fare']   = 5\n",
    "    titanic_test.loc[ titanic_test['Fare'] > 53.1, 'Fare'] = 6\n",
    "    titanic_test['Fare'] = titanic_test['Fare'].astype(int)\n",
    "    \n",
    "    # Additional coloumns\n",
    "    \n",
    "    titanic_test['Sex_Title']= titanic_test['Title'] + titanic_test['Sex']\n",
    "    \n",
    "    return titanic_test \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your test data here under test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test = data_preprocessing(\"titanic_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we don't know if the test dataset will include the target variable, we will fit the dataset for both scenarios\n",
    "\n",
    "X_train = titanic_data.drop(\"Survived\", axis=1)\n",
    "Y_train = titanic_data[\"Survived\"]\n",
    "if \"Survived\" in titanic_test.columns:\n",
    "    Y_test = titanic_test[\"Survived\"]\n",
    "    X_test = titanic_test.drop([\"PassengerId\", \"Survived\"], axis = 1).copy()\n",
    "else:\n",
    "    X_test = titanic_test.drop(\"PassengerId\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scaling to the test data\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_scale.fit(X_train)\n",
    "X_train = minmax_scale.transform(X_train)\n",
    "X_test = minmax_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,Y_train)\n",
    "Y_pred = knn.predict(X_test)  \n",
    "acc_knn = round(knn.score(X_test, Y_test) * 100, 2)\n",
    "print(acc_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
